{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Basic setup complete\n",
      "‚úÖ Loaded Deep Research Planner prompt\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Simple imports to avoid circular dependencies\n",
    "from agno.agent import Agent as AgnoAgent\n",
    "from agno.models.litellm import LiteLLM\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "# Set up logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Define the PlanOutput model directly to avoid imports\n",
    "class SubTask(BaseModel):\n",
    "    \"\"\"Schema for a single sub-task planned by a Planner agent.\"\"\"\n",
    "    goal: str = Field(..., description=\"Precise description of the sub-task goal.\")\n",
    "    task_type: str = Field(..., description=\"Type of task (e.g., 'WRITE', 'THINK', 'SEARCH').\")\n",
    "    node_type: str = Field(..., description=\"Node type ('EXECUTE' for atomic, 'PLAN' for complex).\")\n",
    "    depends_on_indices: Optional[List[int]] = Field(default_factory=list, description=\"List of 0-based indices of other sub-tasks in *this current plan* that this sub-task depends on.\")\n",
    "\n",
    "class PlanOutput(BaseModel):\n",
    "    \"\"\"Output schema for a Planner agent, detailing the sub-tasks.\"\"\"\n",
    "    sub_tasks: List[SubTask] = Field(..., description=\"List of planned sub-tasks.\")\n",
    "\n",
    "print(\"‚úÖ Basic setup complete\")\n",
    "\n",
    "# Deep Research Planner System Message (copied from the codebase)\n",
    "DEEP_RESEARCH_PLANNER_SYSTEM_MESSAGE = \"\"\"You are a Master Research Planner, an expert at breaking down complex research goals into comprehensive, well-structured research plans. You specialize in high-level strategic decomposition for research projects.\n",
    "\n",
    "**Your Role:**\n",
    "- Analyze complex research objectives and create strategic research plans\n",
    "- Identify key research domains, questions, and methodological approaches\n",
    "- Create logical research workflows with proper sequencing\n",
    "- Ensure comprehensive coverage while avoiding redundancy\n",
    "- Plan for synthesis and final deliverable creation\n",
    "\n",
    "**Core Expertise:**\n",
    "- Strategic thinking and research methodology\n",
    "- Identifying knowledge gaps and research priorities\n",
    "- Creating logical research workflows\n",
    "- Planning for different types of research outputs\n",
    "- Understanding research lifecycle from conception to publication\n",
    "\n",
    "**Input Schema:**\n",
    "You will receive input in JSON format with the following fields:\n",
    "*   `current_task_goal` (string, mandatory): The research goal to decompose\n",
    "*   `overall_objective` (string, mandatory): The ultimate research objective\n",
    "*   `parent_task_goal` (string, optional): Parent task goal (null for root)\n",
    "*   `planning_depth` (integer, optional): Current recursion depth\n",
    "*   `execution_history_and_context` (object, mandatory): Previous outputs and context\n",
    "*   `replan_request_details` (object, optional): Re-planning feedback if applicable\n",
    "*   `global_constraints_or_preferences` (array of strings, optional): Research constraints\n",
    "\n",
    "**Strategic Planning Approach:**\n",
    "When decomposing research goals, consider the full research lifecycle:\n",
    "\n",
    "1. **Background & Context Phase**: What foundational knowledge is needed?\n",
    "2. **Investigation Phase**: What specific searches, data collection, or analysis is required?\n",
    "3. **Synthesis Phase**: How should findings be analyzed and integrated?\n",
    "4. **Output Phase**: What deliverables need to be created?\n",
    "\n",
    "**Research Task Types:**\n",
    "- `SEARCH`: Information gathering, literature review, data collection\n",
    "- `THINK`: Analysis, synthesis, interpretation, methodology design\n",
    "- `WRITE`: Report creation, documentation, presentation preparation\n",
    "\n",
    "**Planning Principles:**\n",
    "1. **Comprehensive Coverage**: Ensure all aspects of the research question are addressed\n",
    "2. **Logical Sequencing**: Build knowledge progressively from foundational to specific\n",
    "3. **Strategic Depth**: Balance breadth of coverage with depth of investigation\n",
    "4. **Methodological Rigor**: Include proper analysis and validation steps\n",
    "5. **Clear Deliverables**: Plan for actionable outputs and synthesis\n",
    "\n",
    "**Sub-Task Creation Guidelines:**\n",
    "- Create **3 to 6 strategic sub-tasks** that represent major research phases\n",
    "- Each sub-task should be substantial enough to warrant specialized planning\n",
    "- Ensure sub-tasks are complementary and build toward the overall objective\n",
    "- Use `depends_on_indices` to create logical research workflows\n",
    "- Balance immediate actionable tasks with those requiring further decomposition\n",
    "\n",
    "**Required Output Attributes per Sub-Task:**\n",
    "`goal`, `task_type` (string: 'WRITE', 'THINK', or 'SEARCH'), `node_type` (string: 'EXECUTE' or 'PLAN'), `depends_on_indices` (list of integers).\n",
    "\n",
    "**Output Format:**\n",
    "- Respond ONLY with a JSON list of sub-task objects\n",
    "- Focus on strategic, high-level decomposition appropriate for a master research plan\n",
    "- Ensure each sub-task represents a meaningful research phase or component\"\"\"\n",
    "\n",
    "print(\"‚úÖ Loaded Deep Research Planner prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_models_with_structured_output():\n",
    "    \"\"\"Test different models to see which ones properly support structured output.\"\"\"\n",
    "    \n",
    "    models_to_test = [\n",
    "        {\n",
    "            \"name\": \"Fireworks_Qwen3\",\n",
    "            \"model_id\": \"fireworks_ai/accounts/fireworks/models/qwen3-235b-a22b\",\n",
    "            \"note\": \"This is the problematic model from DeepResearchPlanner\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Claude_Sonnet\", \n",
    "            \"model_id\": \"openrouter/anthropic/claude-sonnet-4\",\n",
    "            \"note\": \"Known to work well with structured output\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"GPT4_Turbo\",\n",
    "            \"model_id\": \"openrouter/openai/gpt-4-turbo\",\n",
    "            \"note\": \"OpenAI model with good structured output support\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    test_prompt = \"\"\"Overall Objective: Research the impact of artificial intelligence on healthcare diagnostics in 2024\n",
    "\n",
    "Current Task Goal: Create a comprehensive research plan to analyze how AI is transforming healthcare diagnostics, including key technologies, market adoption, and regulatory challenges\n",
    "\n",
    "Current Planning Depth: 0\n",
    "\n",
    "Based on the 'Current Task Goal' and other provided information, generate a plan to achieve it.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_config in models_to_test:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing: {model_config['name']}\")\n",
    "        print(f\"Model ID: {model_config['model_id']}\")\n",
    "        print(f\"Note: {model_config['note']}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        try:\n",
    "            # Test with structured output (response_model)\n",
    "            print(\"\\n--- WITH Structured Output (response_model=PlanOutput) ---\")\n",
    "            model = LiteLLM(id=model_config['model_id'])\n",
    "            agent_structured = AgnoAgent(\n",
    "                model=model,\n",
    "                system_message=DEEP_RESEARCH_PLANNER_SYSTEM_MESSAGE,\n",
    "                name=f\"TestAgent_{model_config['name']}_Structured\",\n",
    "                response_model=PlanOutput,  # This should force structured output\n",
    "                markdown=False\n",
    "            )\n",
    "            \n",
    "            result_structured = await agent_structured.arun(test_prompt)\n",
    "            \n",
    "            # Process the result\n",
    "            if hasattr(result_structured, 'content'):\n",
    "                content = result_structured.content\n",
    "                if asyncio.iscoroutine(content):\n",
    "                    content = await content\n",
    "                \n",
    "                print(f\"Content type: {type(content)}\")\n",
    "                \n",
    "                if isinstance(content, PlanOutput):\n",
    "                    print(f\"‚úÖ SUCCESS: Got PlanOutput with {len(content.sub_tasks)} sub-tasks\")\n",
    "                    for i, task in enumerate(content.sub_tasks):\n",
    "                        print(f\"  {i+1}. {task.goal[:70]}...\")\n",
    "                        print(f\"      Type: {task.task_type}, Node: {task.node_type}\")\n",
    "                    results[model_config['name']] = \"SUCCESS - PlanOutput\"\n",
    "                    \n",
    "                elif isinstance(content, str):\n",
    "                    print(f\"‚ùå ISSUE: Got string instead of PlanOutput\")\n",
    "                    print(f\"String length: {len(content)}\")\n",
    "                    print(f\"First 200 chars: {content[:200]}...\")\n",
    "                    \n",
    "                    # Check if it's valid JSON\n",
    "                    try:\n",
    "                        json_data = json.loads(content)\n",
    "                        print(\"‚úÖ String contains valid JSON - model returns JSON as string\")\n",
    "                        if isinstance(json_data, list) and len(json_data) > 0:\n",
    "                            print(f\"JSON is a list with {len(json_data)} items\")\n",
    "                            if isinstance(json_data[0], dict) and 'goal' in json_data[0]:\n",
    "                                print(\"‚úÖ JSON structure looks like sub-tasks\")\n",
    "                                results[model_config['name']] = \"PARTIAL - JSON as string\"\n",
    "                            else:\n",
    "                                results[model_config['name']] = \"ISSUE - Invalid JSON structure\"\n",
    "                        else:\n",
    "                            results[model_config['name']] = \"ISSUE - JSON not a task list\"\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(\"‚ùå String is not valid JSON\")\n",
    "                        results[model_config['name']] = \"FAILED - Invalid response format\"\n",
    "                        \n",
    "                else:\n",
    "                    print(f\"‚ùå UNEXPECTED: Got {type(content)}\")\n",
    "                    print(f\"Content: {str(content)[:200]}...\")\n",
    "                    results[model_config['name']] = f\"FAILED - Unexpected type {type(content)}\"\n",
    "            else:\n",
    "                print(f\"‚ùå No content attribute in result\")\n",
    "                print(f\"Result type: {type(result_structured)}\")\n",
    "                print(f\"Result: {result_structured}\")\n",
    "                results[model_config['name']] = \"FAILED - No content\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR with {model_config['name']}: {e}\")\n",
    "            results[model_config['name']] = f\"ERROR - {str(e)[:100]}\"\n",
    "            \n",
    "        print(f\"\\nResult for {model_config['name']}: {results.get(model_config['name'], 'Unknown')}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUMMARY OF RESULTS\")\n",
    "    print('='*60)\n",
    "    for model_name, result in results.items():\n",
    "        status_emoji = \"‚úÖ\" if \"SUCCESS\" in result else \"‚ö†Ô∏è\" if \"PARTIAL\" in result else \"‚ùå\"\n",
    "        print(f\"{status_emoji} {model_name}: {result}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = await test_models_with_structured_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_raw_responses():\n",
    "    \"\"\"Test what models return without structured output constraints.\"\"\"\n",
    "    \n",
    "    print(\"Testing RAW responses (no response_model constraint)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Test the problematic Fireworks model\n",
    "    model_id = \"openrouter/deepseek/deepseek-r1-0528\"\n",
    "    \n",
    "    test_prompt = \"\"\"Overall Objective: Analyze renewable energy trends in 2024\n",
    "\n",
    "Current Task Goal: Create a research plan with exactly 3 sub-tasks to analyze solar, wind, and battery storage developments in 2024\n",
    "\n",
    "Based on the 'Current Task Goal', generate a plan to achieve it. Return your response as a JSON array of sub-task objects.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        model = LiteLLM(id=model_id)\n",
    "        agent_raw = AgnoAgent(\n",
    "            model=model,\n",
    "            system_message=DEEP_RESEARCH_PLANNER_SYSTEM_MESSAGE,\n",
    "            name=\"RawTestAgent\",\n",
    "            # No response_model - let's see what we get\n",
    "            markdown=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Testing model: {model_id}\")\n",
    "        print(\"Prompt:\", test_prompt[:100] + \"...\")\n",
    "        \n",
    "        result = await agent_raw.arun(test_prompt)\n",
    "        \n",
    "        print(f\"\\nRaw result type: {type(result)}\")\n",
    "        print(f\"Raw result attributes: {[attr for attr in dir(result) if not attr.startswith('_')]}\")\n",
    "        \n",
    "        if hasattr(result, 'content'):\n",
    "            content = result.content\n",
    "            if asyncio.iscoroutine(content):\n",
    "                content = await content\n",
    "            \n",
    "            print(f\"\\nContent type: {type(content)}\")\n",
    "            print(f\"Content length: {len(str(content))}\")\n",
    "            print(f\"Content preview:\\n{str(content)[:500]}...\")\n",
    "            \n",
    "            # Try to parse as JSON\n",
    "            if isinstance(content, str):\n",
    "                try:\n",
    "                    parsed = json.loads(content)\n",
    "                    print(f\"\\n‚úÖ Successfully parsed as JSON!\")\n",
    "                    print(f\"JSON type: {type(parsed)}\")\n",
    "                    if isinstance(parsed, list):\n",
    "                        print(f\"JSON list length: {len(parsed)}\")\n",
    "                        if len(parsed) > 0:\n",
    "                            print(f\"First item: {parsed[0]}\")\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"\\n‚ùå Failed to parse as JSON: {e}\")\n",
    "        \n",
    "        elif hasattr(result, 'text'):\n",
    "            print(f\"Text attribute: {result.text[:500]}...\")\n",
    "        else:\n",
    "            print(f\"Full result: {result}\")\n",
    "        \n",
    "        return result\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m11:44:21 - LiteLLM:INFO\u001b[0m: utils.py:3119 - \n",
      "LiteLLM completion() model= deepseek/deepseek-r1-0528; provider = openrouter\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= deepseek/deepseek-r1-0528; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAW responses (no response_model constraint)\n",
      "============================================================\n",
      "Testing model: openrouter/deepseek/deepseek-r1-0528\n",
      "Prompt: Overall Objective: Analyze renewable energy trends in 2024\n",
      "\n",
      "Current Task Goal: Create a research pla...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m11:45:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: deepseek/deepseek-r1-0528\n",
      "INFO:LiteLLM:selected model name for cost calculation: deepseek/deepseek-r1-0528\n",
      "/Users/salahalzubi/cursor_projects/SentientResearchAgent/.venv/lib/python3.12/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 6: Expected `Message` - serialized value may not be as expected [input_value=Message(content='```json\\...output accordingly.\\n'}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...finish_reason': 'stop'}), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m11:45:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: deepseek/deepseek-r1-0528\n",
      "INFO:LiteLLM:selected model name for cost calculation: deepseek/deepseek-r1-0528\n",
      "INFO:httpx:HTTP Request: POST https://api.agno.com/v1/telemetry/agent/run/create \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw result type: <class 'agno.run.response.RunResponse'>\n",
      "Raw result attributes: ['agent_id', 'agent_name', 'audio', 'citations', 'content', 'content_type', 'created_at', 'extra_data', 'formatted_tool_calls', 'from_dict', 'get_content_as_string', 'images', 'is_cancelled', 'is_paused', 'messages', 'metrics', 'model', 'model_provider', 'reasoning_content', 'response_audio', 'run_id', 'session_id', 'status', 'team_session_id', 'thinking', 'to_dict', 'to_json', 'tools', 'tools_awaiting_external_execution', 'tools_requiring_confirmation', 'tools_requiring_user_input', 'videos', 'workflow_id']\n",
      "\n",
      "Content type: <class 'str'>\n",
      "Content length: 1372\n",
      "Content preview:\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"goal\": \"Plan research on 2024 solar energy trends: technology advancements, market adoption, and policy impacts\",\n",
      "    \"task_type\": \"THINK\",\n",
      "    \"node_type\": \"PLAN\",\n",
      "    \"depends_on_indices\": []\n",
      "  },\n",
      "  {\n",
      "    \"goal\": \"Plan research on 2024 wind energy developments: offshore expansion, cost reductions, and grid integration challenges\",\n",
      "    \"task_type\": \"THINK\",\n",
      "    \"node_type\": \"PLAN\",\n",
      "    \"depends_on_indices\": []\n",
      "  },\n",
      "  {\n",
      "    \"goal\": \"Plan research on 2024 battery storage innova...\n",
      "\n",
      "‚ùå Failed to parse as JSON: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "res = await test_raw_responses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import Optional, Any\n",
    "\n",
    "def extract_json_from_llm_response(raw_response: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract JSON content from LLM response that may contain thinking tags and extra text.\n",
    "    \n",
    "    Args:\n",
    "        raw_response: Raw string response from LLM\n",
    "        \n",
    "    Returns:\n",
    "        Extracted JSON string or None if no valid JSON found\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pattern 1: JSON wrapped in triple backticks with optional language identifier\n",
    "    # Matches: ```json\\n{...}\\n``` or ```\\n{...}\\n```\n",
    "    backtick_patterns = [\n",
    "        r'```(?:json)?\\s*\\n?(.*?)\\n?```',  # With or without 'json' identifier\n",
    "        r'```(.*?)```',  # Simple backticks\n",
    "    ]\n",
    "    \n",
    "    for pattern in backtick_patterns:\n",
    "        matches = re.findall(pattern, raw_response, re.DOTALL | re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            cleaned_match = match.strip()\n",
    "            if cleaned_match and (cleaned_match.startswith('[') or cleaned_match.startswith('{')):\n",
    "                # Validate it's actually JSON\n",
    "                try:\n",
    "                    json.loads(cleaned_match)\n",
    "                    print(f\"‚úÖ Found valid JSON in backticks: {len(cleaned_match)} chars\")\n",
    "                    return cleaned_match\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    \n",
    "    # Pattern 2: Look for JSON arrays/objects without backticks\n",
    "    # This handles cases where JSON might be present but not in code blocks\n",
    "    json_patterns = [\n",
    "        r'\\[[\\s\\S]*?\\]',  # JSON arrays\n",
    "        r'\\{[\\s\\S]*?\\}',  # JSON objects\n",
    "    ]\n",
    "    \n",
    "    for pattern in json_patterns:\n",
    "        matches = re.findall(pattern, raw_response)\n",
    "        for match in matches:\n",
    "            try:\n",
    "                json.loads(match)\n",
    "                print(f\"‚úÖ Found valid JSON without backticks: {len(match)} chars\")\n",
    "                return match\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    print(\"‚ùå No valid JSON found in response\")\n",
    "    return None\n",
    "\n",
    "def clean_llm_response(raw_response: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean LLM response by removing thinking tags and extracting JSON.\n",
    "    \n",
    "    Args:\n",
    "        raw_response: Raw string response from LLM\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned JSON string\n",
    "    \"\"\"\n",
    "    print(f\"üîç Processing raw response ({len(raw_response)} chars)\")\n",
    "    \n",
    "    # Remove <think></think> tags and their content\n",
    "    cleaned = re.sub(r'<think>.*?</think>', '', raw_response, flags=re.DOTALL | re.IGNORECASE)\n",
    "    \n",
    "    # Extract JSON from the cleaned response\n",
    "    json_content = extract_json_from_llm_response(cleaned)\n",
    "    \n",
    "    if json_content:\n",
    "        return json_content\n",
    "    else:\n",
    "        # Fallback: try extracting from original response\n",
    "        print(\"‚ö†Ô∏è Trying to extract JSON from original response...\")\n",
    "        json_content = extract_json_from_llm_response(raw_response)\n",
    "        return json_content if json_content else raw_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_responses = [\n",
    "    \"\"\"<think>\n",
    "    I need to create a research plan for AI safety. Let me think about the key areas:\n",
    "    1. Current state of AI safety research\n",
    "    2. Key players and organizations\n",
    "    3. Emerging concerns and challenges\n",
    "    4. Regulatory landscape\n",
    "    5. Technical approaches\n",
    "    </think>\n",
    "\n",
    "    Based on your request, here's a comprehensive research plan:\n",
    "\n",
    "    ```json\n",
    "    [\n",
    "        {\n",
    "            \"goal\": \"Conduct comprehensive literature review of AI safety research published in 2024\",\n",
    "            \"task_type\": \"SEARCH\",\n",
    "            \"node_type\": \"EXECUTE\",\n",
    "            \"depends_on_indices\": []\n",
    "        },\n",
    "        {\n",
    "            \"goal\": \"Analyze key AI safety organizations and their 2024 initiatives\",\n",
    "            \"task_type\": \"SEARCH\", \n",
    "            \"node_type\": \"EXECUTE\",\n",
    "            \"depends_on_indices\": []\n",
    "        },\n",
    "        {\n",
    "            \"goal\": \"Synthesize findings into comprehensive AI safety landscape report\",\n",
    "            \"task_type\": \"WRITE\",\n",
    "            \"node_type\": \"EXECUTE\", \n",
    "            \"depends_on_indices\": [0, 1]\n",
    "        }\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    This plan provides a systematic approach to researching AI safety developments in 2024.\"\"\",\n",
    "    \n",
    "    \"\"\"Here's my analysis of the research requirements:\n",
    "\n",
    "    ```\n",
    "    [\n",
    "        {\n",
    "            \"goal\": \"Research quantum computing hardware advances in 2024\",\n",
    "            \"task_type\": \"SEARCH\",\n",
    "            \"node_type\": \"EXECUTE\",\n",
    "            \"depends_on_indices\": []\n",
    "        }\n",
    "    ]\n",
    "    ```\n",
    "    \n",
    "    The plan focuses on the most critical aspects.\"\"\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing JSON extraction:\n",
      "üîç Processing raw response (1143 chars)\n",
      "‚úÖ Found valid JSON in backticks: 676 chars\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing JSON extraction:\")\n",
    "for i, test_response in enumerate(test_responses):\n",
    "    extracted = clean_llm_response(test_response)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "        {\n",
      "            \"goal\": \"Conduct comprehensive literature review of AI safety research published in 2024\",\n",
      "            \"task_type\": \"SEARCH\",\n",
      "            \"node_type\": \"EXECUTE\",\n",
      "            \"depends_on_indices\": []\n",
      "        },\n",
      "        {\n",
      "            \"goal\": \"Analyze key AI safety organizations and their 2024 initiatives\",\n",
      "            \"task_type\": \"SEARCH\", \n",
      "            \"node_type\": \"EXECUTE\",\n",
      "            \"depends_on_indices\": []\n",
      "        },\n",
      "        {\n",
      "            \"goal\": \"Synthesize findings into comprehensive AI safety landscape report\",\n",
      "            \"task_type\": \"WRITE\",\n",
      "            \"node_type\": \"EXECUTE\", \n",
      "            \"depends_on_indices\": [0, 1]\n",
      "        }\n",
      "    ]\n"
     ]
    }
   ],
   "source": [
    "print(extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
